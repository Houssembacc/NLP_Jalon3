{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gov4x2PUmP5_",
        "outputId": "c7e7865f-517c-4e0f-8e2d-aa8d6effa604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textBlob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textBlob) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textBlob) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textBlob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textBlob) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textBlob) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install  textBlob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGvwTbXt1Wvt",
        "outputId": "c62ab666-9f8d-491f-9958-955501f2c269"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 40.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cd3bpuE0l9M",
        "outputId": "dce83139-1e67-49e9-e012-db19c2d01d49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td2zGKMv0ubO",
        "outputId": "4946e467-7275-4e5a-d6d6-e8d5b24568b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    \n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    \n",
        "    return \" \".join(lemmatized_text_list)\n",
        "\n",
        "\"\"\"def lemmatize_text(text):\n",
        "    lemmatized_text_list = list()\n",
        "\n",
        "    for word in text.split():\n",
        "        lemmatized_text_list.append(lemmatizer.lemmatize(word, \"v\")) # Lemmatise verbs\n",
        "\n",
        "    return \" \".join(lemmatized_text_list)\"\"\"\n",
        "\n",
        "\"\"\"def lemmatize_text(text):\n",
        "    \n",
        "    doc = nlp.pipe(text, batch_size=32, n_process=8)\n",
        "    \n",
        "    return \" \".join([w.lemma_ if w.lemma_ != '-PRON-' else w.lower_ for w in doc])\"\"\"\n",
        "\n",
        "    #return \" \".join([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "PUyWQP3V00x4",
        "outputId": "81ca9133-a614-4e72-b0f7-3d17bcfc81a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['parser', 'tagger', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def lemmatize_text(text):\\n    \\n    doc = nlp.pipe(text, batch_size=32, n_process=8)\\n    \\n    return \" \".join([w.lemma_ if w.lemma_ != \\'-PRON-\\' else w.lower_ for w in doc])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--SUPhcs07at",
        "outputId": "83d1b163-b602-42a0-b943-10a0b225a49c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contraction_text(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "2nIk8Q-L0_My"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\""
      ],
      "metadata": {
        "id": "slTsyAGT07ki"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "xXrhqrJF07pq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])"
      ],
      "metadata": {
        "id": "U9ub7UX61L8d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \n",
        "    # Tokenize review\n",
        "    text = tokenize_text(text)\n",
        "    \n",
        "    # Lemmatize review\n",
        "    text = lemmatize_text(text)\n",
        "    \n",
        "    # Normalize review\n",
        "    text = normalize_text(text)\n",
        "    \n",
        "    # Remove contractions\n",
        "    text = contraction_text(text)\n",
        "\n",
        "    # Get negative tokens\n",
        "    text = get_negative_token(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    text = remove_stopwords(text)\n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "8YmaGzr21NSC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jalon 3"
      ],
      "metadata": {
        "id": "rbyjg1zUpkcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file_name=\"/content/drive/MyDrive/TP-NLP/Jalon3/vectorizer.bin\"\n",
        "with (open(file_name, \"rb\")) as f:\n",
        "    while True:\n",
        "        try:\n",
        "            vectorizer=pickle.load(f)\n",
        "        except EOFError:\n",
        "            break\n",
        "vectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYOL7PlImWu-",
        "outputId": "22a2994b-fc53-43e3-d3ed-1cd9c8306956"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(max_df=0.7, min_df=0.01)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "file_name=\"/content/drive/MyDrive/TP-NLP/Jalon3/model_nmf.bin\"\n",
        "\n",
        "with (open(file_name, \"rb\")) as f:\n",
        "    while True:\n",
        "        try:\n",
        "            nmf_model=pickle.load(f)\n",
        "        except EOFError:\n",
        "            break\n",
        "nmf_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9RMKygKn2v1",
        "outputId": "77b84264-c8e9-4369-b39e-0a1d23e19aa7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(n_components=15)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=preprocess_text(\"I am good\")\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NrJ1JXUirOqS",
        "outputId": "2ebe1b1e-9780-4d35-da82-38da1ff3f77e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "liste=[]\n",
        "liste.append(\"I am good\")\n",
        "x=vectorizer.transform(liste)\n",
        "y=nmf_model.transform(x)\n",
        "print(y)\n",
        "ind=[]\n",
        "argsort=np.argsort(y, axis=1)\n",
        "print(argsort)\n",
        "for i in range(2):   \n",
        "      ind.append(argsort[0][len(argsort[0])-(i+1)])\n",
        "ind\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnZ2mrWTxX8_",
        "outputId": "fa0453d3-b30a-4a41-985e-11d55ecb85d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00000000e+00 0.00000000e+00 4.60482251e-05 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.29701667e-01]]\n",
            "[[ 0  1  3  4  5  6  7  8  9 10 11 12 13  2 14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fonction de prediction\n",
        "\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "interpretation_topi={\n",
        "    'topic0' : 'TICKET PAYEMENT',\n",
        "    'topic1' : 'OVERLOADED RESTAURANT',\n",
        "    'topic2' : 'PIZZA/DELIVERY',\n",
        "    'topic3' : 'PRICES/ONLINE-ORDERING',\n",
        "    'topic4' : 'FOOD/SERVICE',\n",
        "    'topic5' : 'TABLE/BOTHERING',\n",
        "    'topic6' : 'SERVICE',\n",
        "    'topic7' : 'TEMPS D ATTENTE',\n",
        "    'topic8' : 'CHICKEN',\n",
        "    'topic9' : 'BEER/BAR',\n",
        "    'topic10' : 'BAD PLACE TO EAT',\n",
        "    'topic11' : 'SUSHI',\n",
        "    'topic12' : 'LUNCH/SANDWICH',\n",
        "    'topic13' : 'AMBIANCE',\n",
        "    'topic14' : 'WAITER'\n",
        "}\n",
        "def prediction_text(vectorizer,nmf_model,n_topic,text):\n",
        "  topic_list=['TICKET PAYEMENT','OVERLOADED RESTAURANT','PIZZA/DELIVERY','PRICES/ONLINE-ORDERING','FOOD/SERVICE','TABLE/BOTHERING','SERVICE','TEMPS D ATTENTE','CHICKEN', 'BEER/BAR','BAD PLACE TO EAT','SUSHI',\n",
        "     'LUNCH/SANDWICH',\n",
        "     'AMBIANCE',\n",
        "     'WAITER']\n",
        "\n",
        "  txt=TextBlob(text)\n",
        "  polarity=txt.polarity\n",
        "  liste=[]\n",
        "  text_cleaned=preprocess_text(text)\n",
        "  liste.append(text_cleaned)\n",
        "  idx_l=[]\n",
        "  if (polarity<0):\n",
        "    text_cleaned=preprocess_text(text)\n",
        "    liste.append(text_cleaned)\n",
        "    x=vectorizer.transform(liste)\n",
        "    y=nmf_model.transform(x)\n",
        "    ind=[]\n",
        "    argsort=np.argsort(y, axis=1)\n",
        "    print(argsort)\n",
        "    for i in range(n_topic):   \n",
        "      ind.append(argsort[0][len(argsort[0])-(i+1)])\n",
        "    \n",
        "    for i in ind:\n",
        "      idx_l.append(topic_list[i])\n",
        "\n",
        "  return polarity,idx_l\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "3RC8QU1gort5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p,l=prediction_text(vectorizer,nmf_model,2,\"I am bad\")\n",
        "print(\"polarity is : \",p)\n",
        "print(\"Topics are : \",l)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-LF8jfN_1Kt",
        "outputId": "244415c7-c741-4aa6-aa5b-dbddea91952d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  3  5  6  7  8  9 10 11 14  2  1 12 13  4]\n",
            " [ 0  3  5  6  7  8  9 10 11 14  2  1 12 13  4]]\n",
            "polarity is :  -0.6999999999999998\n",
            "Topics are :  ['FOOD/SERVICE', 'AMBIANCE']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBSj5Yu8_6OU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}